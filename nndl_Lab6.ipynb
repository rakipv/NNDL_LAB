{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1. Build a BPN classifier for any kaggle dataset."
      ],
      "metadata": {
        "id": "UgNJBFJhY27G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "db = np.loadtxt(\"duke-breast-cancer.txt\")\n",
        "print(\"Database raw shape (%s,%s)\" % np.shape(db))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUYadN0XhwEl",
        "outputId": "440d9939-aa46-4489-d4c4-ef6dd8665067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database raw shape (86,7130)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Now we have to shuffle it and then split it into training 90% and testing 10% so that the network can train itself better. "
      ],
      "metadata": {
        "id": "lwLtE_rairnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.shuffle(db)\n",
        "y = db[:, 0]\n",
        "x = np.delete(db, [0], axis=1)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "print(np.shape(x_train),np.shape(x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKmBGRGSiwl-",
        "outputId": "6340b5b8-4144-4051-f800-1bb3eeaab46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(77, 7129) (9, 7129)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to create the hidden layer vector, weight's matrix, output layer vector and the hidden weight's matrix. We choose the hidden layer to be made of a number of [72] hidden perceptrons. The output layer needs to have a number of perceptrons equal to the number of classes."
      ],
      "metadata": {
        "id": "5ycZkhWAhwV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer = np.zeros(72)\n",
        "weights = np.random.random((len(x[0]), 72))\n",
        "output_layer = np.zeros(2)\n",
        "hidden_weights = np.random.random((72, 2))"
      ],
      "metadata": {
        "id": "aHil8jvXhwV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To continue we need to implement:\n",
        "\n",
        "    1. Sum function\n",
        "    2. Activation function\n",
        "    3. SoftMax function\n",
        "    4. Recalculate Weights function\n",
        "    5. Back-propagation function\n"
      ],
      "metadata": {
        "id": "vUVZVJjBhwV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sum function**"
      ],
      "metadata": {
        "id": "fcFIaFqFhwV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sum for [i]th perceptron from the layer"
      ],
      "metadata": {
        "id": "4EMnJGBDhwV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_function(weights, index_locked_col, x):\n",
        "    result = 0\n",
        "    for i in range(0, len(x)):\n",
        "        result += x[i] * weights[i][index_locked_col]\n",
        "    return result"
      ],
      "metadata": {
        "id": "Ne-7a9eqhwV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Activation function**"
      ],
      "metadata": {
        "id": "3MTRPAxyhwV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "activation for the [i]th perceptron from the layer. "
      ],
      "metadata": {
        "id": "7otk8WpChwV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activate_layer(layer, weights, x):\n",
        "    for i in range(0, len(layer)):\n",
        "        layer[i] = 1.7159 * np.tanh(2.0 * sum_function(weights, i, x) / 3.0)"
      ],
      "metadata": {
        "id": "jq2twewchwV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SoftMax function**"
      ],
      "metadata": {
        "id": "_Afxv4uohwV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax function, or normalized exponential function, is a generalization of the logistic function that \"squashes\" a K-dimensional vector z of arbitrary real values to a K-dimensional vector σ ( z ) of real values in the range (0, 1) that add up to 1."
      ],
      "metadata": {
        "id": "DODzvUvQhwV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_max(layer):\n",
        "    soft_max_output_layer = np.zeros(len(layer))\n",
        "    for i in range(0, len(layer)):\n",
        "        denominator = 0\n",
        "        for j in range(0, len(layer)):\n",
        "            denominator += np.exp(layer[j] - np.max(layer))\n",
        "        soft_max_output_layer[i] = np.exp(layer[i] - np.max(layer)) / denominator\n",
        "    return soft_max_output_layer"
      ],
      "metadata": {
        "id": "mun_-yXjhwV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recalculate Weights function**"
      ],
      "metadata": {
        "id": "FOnKbyqahwV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we tune the network weights and hidden weights matrix. We are going to use this inside the back propagation function."
      ],
      "metadata": {
        "id": "az5y-si_hwV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recalculate_weights(learning_rate, weights, gradient, activation):\n",
        "    for i in range(0, len(weights)):\n",
        "        for j in range(0, len(weights[i])):\n",
        "            weights[i][j] = (learning_rate * gradient[j] * activation[i]) + weights[i][j]"
      ],
      "metadata": {
        "id": "eDB7XFwKhwV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Back-propagation function**"
      ],
      "metadata": {
        "id": "95GaAKPahwWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we find out the output layer gradient and the hidden layer gradient to recalculate the network weights."
      ],
      "metadata": {
        "id": "XvgaFSOghwWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_propagation(hidden_layer, output_layer, one_hot_encoding, learning_rate, x):\n",
        "    output_derivative = np.zeros(2)\n",
        "    output_gradient = np.zeros(2)\n",
        "    for i in range(0, len(output_layer)):\n",
        "        output_derivative[i] = (1.0 - output_layer[i]) * output_layer[i]\n",
        "    for i in range(0, len(output_layer)):\n",
        "        output_gradient[i] = output_derivative[i] * (one_hot_encoding[i] - output_layer[i])\n",
        "    hidden_derivative = np.zeros(72)\n",
        "    hidden_gradient = np.zeros(72)\n",
        "    for i in range(0, len(hidden_layer)):\n",
        "        hidden_derivative[i] = (1.0 - hidden_layer[i]) * (1.0 + hidden_layer[i])\n",
        "    for i in range(0, len(hidden_layer)):\n",
        "        sum_ = 0\n",
        "        for j in range(0, len(output_gradient)):\n",
        "            sum_ += output_gradient[j] * hidden_weights[i][j]\n",
        "        hidden_gradient[i] = sum_ * hidden_derivative[i]\n",
        "    recalculate_weights(learning_rate, hidden_weights, output_gradient, hidden_layer)\n",
        "    recalculate_weights(learning_rate, weights, hidden_gradient, x)"
      ],
      "metadata": {
        "id": "Z7iwUTmehwWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we one hot encode\n",
        "\n",
        "converting the categorical data variables to be provided to machine and deep learning algorithms which in turn improve predictions as well as classification accuracy of a model"
      ],
      "metadata": {
        "id": "BZ0_PHZMhwWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoding = np.zeros((2,2))\n",
        "for i in range(0, len(one_hot_encoding)):\n",
        "    one_hot_encoding[i][i] = 1\n",
        "training_correct_answers = 0\n",
        "for i in range(0, len(x_train)):\n",
        "    activate_layer(hidden_layer, weights, x_train[i])\n",
        "    activate_layer(output_layer, hidden_weights, hidden_layer)\n",
        "    output_layer = soft_max(output_layer)\n",
        "    training_correct_answers += 1 if y_train[i] == np.argmax(output_layer) else 0\n",
        "    back_propagation(hidden_layer, output_layer, one_hot_encoding[int(y_train[i])], -1, x_train[i])\n",
        "print(\"Correct answers while learning: %s / %s (Accuracy = %s) \" % (training_correct_answers, len(x_train), \n",
        "                                                                                       training_correct_answers/len(x_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5953adc6-3fb8-47fb-d537-d573b53faf91",
        "id": "N4rb97u8hwWB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct answers while learning: 53 / 77 (Accuracy = 0.6883116883116883) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The accuracy of the test depends on the random generated weight's matrix and the learning rate. Using different learning rates and weight's will generate a different accuracy.\n"
      ],
      "metadata": {
        "id": "fpKcPr6XhwWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testing_correct_answers = 0\n",
        "for i in range(0, len(x_test)):\n",
        "    activate_layer(hidden_layer, weights, x_test[i])\n",
        "    activate_layer(output_layer, hidden_weights, hidden_layer)\n",
        "    output_layer = soft_max(output_layer)\n",
        "    testing_correct_answers += 1 if y_test[i] == np.argmax(output_layer) else 0\n",
        "print(\"Correct answers while testing: %s / %s (Accuracy = %s)\" % (testing_correct_answers, len(x_test),\n",
        "                                                                                     testing_correct_answers/len(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98511131-bc19-4578-ccbc-0fd928525c76",
        "id": "FKCrTznKhwWB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct answers while testing: 5 / 9 (Accuracy = 0.5555555555555556)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Build an RBF classifier for the same dataset and compare the results."
      ],
      "metadata": {
        "id": "ZAVs9mOpY717"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.model_selection import learning_curve"
      ],
      "metadata": {
        "id": "ccXFxerdh7uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "db = pd.read_csv(\"gender_submission.csv\")\n",
        "print(\"Database raw shape (%s,%s)\" % np.shape(db))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "417216e9-c100-4372-ae76-da9f85546600",
        "id": "qCVu9D-bh7uG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4532aa60eac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gender_submission.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Database raw shape (%s,%s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gender_submission.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "train.info()\n",
        "# test.info"
      ],
      "metadata": {
        "id": "5qp0Xl2-h7uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['Title'] = train.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
        "test['Title'] = test.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
        "# normalize the titles\n",
        "normalized_titles = {\n",
        "    \"Capt\":       \"Officer\",\n",
        "    \"Col\":        \"Officer\",\n",
        "    \"Major\":      \"Officer\",\n",
        "    \"Jonkheer\":   \"Royalty\",\n",
        "    \"Don\":        \"Royalty\",\n",
        "    \"Sir\" :       \"Royalty\",\n",
        "    \"Dr\":         \"Officer\",\n",
        "    \"Rev\":        \"Officer\",\n",
        "    \"the Countess\":\"Royalty\",\n",
        "    \"Dona\":       \"Royalty\",\n",
        "    \"Mme\":        \"Mrs\",\n",
        "    \"Mlle\":       \"Miss\",\n",
        "    \"Ms\":         \"Mrs\",\n",
        "    \"Mr\" :        \"Mr\",\n",
        "    \"Mrs\" :       \"Mrs\",\n",
        "    \"Miss\" :      \"Miss\",\n",
        "    \"Master\" :    \"Master\",\n",
        "    \"Lady\" :      \"Royalty\"\n",
        "}\n",
        "\n",
        "train.Title = train.Title.map(normalized_titles)\n",
        "test.Title = train.Title.map(normalized_titles)\n",
        "\n",
        "train.Title.replace(['Mr', 'Miss', 'Mrs', 'Master', 'Officer', 'Royalty'], [1, 2, 3, 4, 5, 6], inplace=True)\n",
        "test.Title.replace(['Mr', 'Miss', 'Mrs', 'Master', 'Officer', 'Royalty'], [1, 2, 3, 4, 5, 6], inplace=True)\n"
      ],
      "metadata": {
        "id": "xPgG-e3Lh7uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the name feature\n",
        "train = train.drop('Name', 1)\n",
        "test = test.drop('Name', 1)\n",
        "# Encoding the features\n",
        "train.Embarked.replace(['S', 'C', 'Q'], [1, 2, 3], inplace=True)\n",
        "train.Sex.replace(['male', 'female'], [1, 2], inplace=True)\n",
        "test.Embarked.replace(['S', 'C', 'Q'], [1, 2, 3], inplace=True)\n",
        "test.Sex.replace(['male', 'female'], [1, 2], inplace=True)\n",
        "\n",
        "# Temporary drop of some columns\n",
        "train = train.drop('Ticket', 1)\n",
        "train = train.drop('Cabin', 1)\n",
        "test = test.drop('Ticket', 1)\n",
        "test = test.drop('Cabin', 1)\n",
        "\n",
        "#Fill NaN values of Age with the mean Age\n",
        "train['Age'] = train['Age'].fillna(train['Age'].sum()/len(train))\n",
        "test['Age'] = test['Age'].fillna(test['Age'].sum()/len(test))\n",
        "#Drop other raw where there is a NaN value\n",
        "train= train.dropna(how='any',axis=0)  \n",
        "test = test.fillna(0)\n",
        "\n",
        "train.info()"
      ],
      "metadata": {
        "id": "f9focAcYh7uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "HNLCpuGZh7uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_train = train.values\n",
        "Data_test = test.values\n",
        "\n",
        "# m = number of input samples\n",
        "m_train = len(Data_train)\n",
        "m_test = len(Data_test)\n",
        "# prediction for training\n",
        "Ytrain = Data_train[:m_train,1]\n",
        "# features for training\n",
        "Xtrain = Data_train[:m_train,2:]\n",
        "# features for testing\n",
        "Xtest = Data_test[:m_test,1:]\n",
        "\n",
        "parameters = {'C': [1, 10, 50, 100,200,300, 1000],'gamma':[0.0001,0.001,0.01,0.1,1.]}\n",
        "#run SVM with rbf kernel\n",
        "rbf_SVM = SVC(kernel='rbf')\n",
        "# ADD CODE: DO THE SAME AS ABOVE FOR RBF KERNEL\n",
        "clf = GridSearchCV(rbf_SVM,parameters,cv=5)\n",
        "clf.fit(Xtrain,Ytrain)\n",
        "\n",
        "print ('\\n RESULTS FOR rbf KERNEL \\n')\n",
        "\n",
        "best_param = clf.best_params_\n",
        "value_best_param_rbf_gammma = best_param['gamma']\n",
        "value_best_param_rbf_c = best_param['C']\n",
        "estim_best = clf.best_estimator_\n",
        "print(\"Best Estimator: \", estim_best)\n",
        "print(\"Best parameters set found:\",best_param)\n",
        "\n",
        "#get training and test error for the best SVM model from CV\n",
        "best_SVM = SVC(C = value_best_param_rbf_c, gamma = value_best_param_rbf_gammma, kernel='rbf')\n",
        "\n",
        "best_SVM.fit(Xtrain,Ytrain)\n",
        "\n",
        "training_error = 1. - best_SVM.score(Xtrain,Ytrain)\n",
        "print(\"Training error: \", training_error)\n",
        "\n",
        "Ytest_predicted = best_SVM.predict(Xtest)\n",
        "\n",
        "print(\"Accuracy =\",1-training_error);"
      ],
      "metadata": {
        "id": "_Ys11cDOh7uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=-1, train_sizes= np.linspace(.1, 1.0, 10)):\n",
        "    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "plot_learning_curve(estim_best,\"SVC learning curves\",Xtrain,Ytrain,cv=5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AzjaGJSUh7uM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}